---
title: "Lab 0 - Revisione di R e predittori ottimali"
author: "Ilaria Prosdocimi"
date: "Semestre 1 - AA 2022/23"
output:
  html_document:
    fig_caption: yes
    theme: flatly #sandstone #spacelab #flatly
    highlight: pygments
    code_folding: show
    toc: TRUE
    toc_depth: 2
    number_sections: TRUE
    toc_float:
      smooth_scroll: FALSE
editor_options: 
  chunk_output_type: console
---

<!-- The default is to show the code - this can be changed in each chunk with the option  -->
<!-- class.source = "fold-hide" -->
<!-- To have the code hidden by default use in the yaml header -->
<!-- code_folding: hide  -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# R: rinfreschiamo un po' la memoria  

Andiamo a leggere un file esterno: dovremo aver premura di salvare il file in una posizione a noi nota nel nostro computer.  
Attenzione: RMarkdown esegue il codice prendendo la cartella dove è salvato il file .Rmd come la working directory. 
Si deve quindi indicare ad R la posizione del file o in maniera "assoluta" o in maniera relativa. 
Per controllare quale è la cartella dove stiamo al momento lavorando si può usare il comando `getwd()`:

```{r}
getwd()
```

Andiamo a leggere un file che contiene informazioni sul prezzo degli affitti nella città di Monaco insieme ad altre caratteristiche degli immobili in affitto. 
Il file è disponibile su Moodle ed è reso disponibile sul [sito](https://www.uni-goettingen.de/de/551625.html) del libro *Regression* di Fahrmeir et al (un ottimo libro che copre aspetti avanzati di regressione lineare e non lineare)

```{r}
mrents <- read.table("./dati/rent99.raw", header = TRUE)
## rent: net rent per month (euros)
## rentsqm: net rent per month  per square meter (euros)
## area Living area square meters)
## year: year of construction
## location: as judged by an expert coded as 1=average location; 2: good location; 3= top location 
## bath: quality of the bathroom 0: standard; 1: premium
## kitchen: quality of the kitchen 0: standard; 1: premium
## cheating: presence of central heating 0: without; 1: with
## district: district in Munich
```

Usiamo questo dataset per rivedere alcuni comandi di R: 


```{r}
#?
summary(mrents)
#?
head(mrents)
```

La principale variabile di interesse è la variabile `rent`: si può visualizzare la distribuzione della variabile (che è continua) facendone  un istogramma: 

```{r}
# ? 
hist(mrents$rent)
```

Notiamo come la variabile sia abbastanza asimmetrica con una coda abbastanza lunga a destra. 

Possiamo calcolare (e visualizzare su un grafico) alcune caratteristiche della variabile: 

```{r}
# ?
mean(mrents$rent)
# ?
median(mrents$rent)
# ?
quantile(mrents$rent)
```

Per visualizzare invece le caratteristiche di variabili categoriali possiamo usare dei barplot: 

```{r}
#?barplot
```


Ora indagare quali siano i fattori che influenzano il prezzo di affitto di una proprietà. Per esempio si possono confrontare alcune caratteristiche degli affitti per diversi tipi di cucina: 

```{r}
# summary
# how to subset in R? use []
```

Usando dei boxplot si possono confrontare visualmente gli affitti in proprietà con cucine o bagni standard e premium: 

```{r}
#?boxplot
```

Le proprietà con cucina o bagno *premium* tendono ad avere affitti più .... 

Possiamo migliorare la leggibilità di questi grafici notando che sebbene `kitchen` sia codificata come variabile numerica nel dataset, essa sia in realtà una variabile categoriale: 

```{r}
# 
```

Qual è invece l'effetto della `location`? 

```{r}
# boxplot
```

I boxplots risultano utili per confrontare variabili continue per diversi livelli di variabili categoriali. Spesso però il nostro interesse è nella relazione tra due o più variabili continue. Per esempio potremmo voler indagare l'effetto che ha l'area di un immobile sul prezzo d'affitto. Per visualizzare questa relazione possiamo usare un grafico di dispersione: 

```{r}
#?
plot(rent ~ area, data = mrents)
plot(mrents$yearc, mrents$rent)
```


# Predittori ottimali e regressione

Prendiamo in considerazione il dataset `cars` già a disposizione in R: 

```{r}
data(cars)
```

Possiamo avere più dettagli sul dataset leggendo l'help-file a disposizione in R:


```{r,eval=FALSE}
help(cars)
```

Diamo un primo sguardo alle caratteristiche del file: 

```{r sumCars}
summary(cars)
```

Vediamo che il dataset è composto da due variabili numeriche: `speed` prende valori tra 4 (mph) and 25 (mph) e `dist` prende valori tra 2 and 120 ft. Abbiamo in totale `r nrow(cars)` osservazioni. 

Possiamo osservare la relazione tra le due variabili con un grafico di dispersione:

```{r plotCars, echo=TRUE}
plot(cars) # notice the axis labels 
```

<!-- This is a comment.  -->
<!-- Notice we could obtain a similar plot simply using  -->
<!-- plot(dist~speed,data=cars)  -->
<!-- or  -->
<!-- plot(cars$speed, cars$dist) -->

La relazione tra le due variabili è positiva: macchine che viaggiano a velocità più alte necessitano di più spazio per fermarsi. Il coefficiente di correlazione (di Pearson) è `r format(cor(cars$speed, cars$dist),digits = 2)`: un valore piuttosto alto. 

Vogliamo ora stimare una retta di regressione che spieghi `dist` (Y) in funzione di `speed` (X). Dobbiamo quindi assumere che la relazione tra $X$ e $Y$ sia lineare: $Y = b_0 + b_1 X$. 
Troveremo poi delle stime per $b_0$ e $b_1$ usando le osservazioni campionarie. 
Nello specifico stimeremo  $b_0$ e $b_1$ in modo che la somma dei quadrati tra i valori osservati ($y_i$) e i valori stimati ($\hat{y}_i$) sia la più piccola possibile. Desideriamo quindi minimizzare (in funzione di $b_0$ e $b_1$) la seguente quantità: 
\[SumSq(b_0,b_1) = \sum_{i=1}^n{ (y_i - (b_0 + b_1{x}_i))^2 }.\]

Per valori arbitrari di $b_0$ e $b_1$ si può osservare la retta specificata dai valori e calcolare il valore di $SumSq(b_0,b_1)$

```{r plotCarsWithLines, class.source = "fold-hide", echo=TRUE}
par(mfrow=c(1,2), bty = "l") ## see ?par
plot(cars, pch = 16, col = "grey60") 
b0_1 <- -16; b1_1 <- 3
abline(b0_1, b1_1, col = 3)
segments(cars$speed, y0 = cars$dist, 
         y1 = b0_1 + b1_1 * cars$speed, 
         lty = 2, col = 3)
title(main=paste("y =",b0_1,"+",b1_1,"x"))
plot(cars, pch = 16, col = "grey60") 
b0_2 <- -17; b1_2 <- 4
abline(b0_2, b1_2, col = 4)
segments(cars$speed, y0 = cars$dist, 
         y1 = b0_2 + b1_2 * cars$speed, 
         lty = 2, col = 4)
title(main=paste("y =",b0_2,"+",b1_2,"x"))
```

Scriviamo ora una funzione che calcoli $SumSq(b_0,b_1)$: 
```{r sumSqComp,class.source = "fold-show"}
computeSumSquares <- function(bs, y, x){
  
}
# computeSumSquares(c(b0_1, b1_1), y = cars$dist,  x = cars$speed)
# computeSumSquares(c(b0_2, b1_2), y = cars$dist,  x = cars$speed)
# computeSumSquares(c(b0_2, b1_2+0.2), y = cars$dist,  x = cars$speed)
# computeSumSquares(c(b0_2+1, b1_2), y = cars$dist,  x = cars$speed)
```

Si noti come i dati del campione $(x,y)$ non cambiano. 

Calcoliamo per esempio la funzione per una griglia di possibili valori di $b_0$ e $b_1$: 


```{r calcSumSq, echo=TRUE}
b0seq <- seq(-20,-5, length.out = 7)
b1seq <- seq(3.4,4.6,length.out = 9)
ssqseq <- matrix(NA, ncol = 9, nrow= 7)

```

La funzione dipende da entrambi i parametri: possiamo fare diversi grafici in 2-D o usare `persp` per fare un grafico in 3D in R:

```{r}
# par(mfrow=c(1,2))
# plot(b0seq,ssqseq[,1], type = "l", ylim = range(ssqseq))
# lines(b0seq,ssqseq[,3], col = 2)
# lines(b0seq,ssqseq[,5], col = 3)
# lines(b0seq,ssqseq[,7], col = 4)
# lines(b0seq,ssqseq[,9], col = 5)
# legend("topright", col=c(1,2,3,4,5), legend = paste("b1 =",b1seq[c(1,3,5,7,9)]), lty = 1)
# plot(b1seq,ssqseq[1,], type = "l", ylim = range(ssqseq))
# lines(b1seq,ssqseq[3,], col = 2)
# lines(b1seq,ssqseq[5,], col = 3)
# lines(b1seq,ssqseq[7,], col = 4)
# legend("topright", col=c(1,2,3,4), legend = paste("b0 =",b0seq[c(1,3,5,7)]), lty = 1)
```



```{r plotSumSq, class.source = "fold-hide"}
# image(b0seq,b1seq,ssqseq)
# par(mfrow=c(1,2))
# persp(b0seq,b1seq,ssqseq, ticktype = "detailed")
# persp(b1seq,b0seq,t(ssqseq), ticktype = "detailed")
# which(ssqseq == min(ssqseq), arr.ind = TRUE)
```

Desideriamo individuare il valori ($b_0$,$b_1$) che minimizzano SumSq($b_0$,$b_1$). Potremmo usare un ottimizzatone numerico, ma in questo caso è possible trovare una soluzione analitica al problema che risulta essere: 
\[\hat{\beta}_1 = \frac{c_{XY}}{s^2_{X}} =  r_{XY} \frac{s_{Y}}{s_{X}},\quad \mbox{and} \quad \hat{\beta}_0 = \overline{y}-\hat{\beta}_1 \overline{x}  \]

```{r byHandCalc}
# rxy <- 
# sx <- 
# mx <- 
# my <- 
# beta1_hat <- 
# beta0_hat <- 
```


Fissiamo $b_0$ - e guardiamo come cambia la somma dei quadrati in funzione di $b_1$: 

```{r minimiseForBeta1}
# 
```

In R tipicamente non faremmo questi conti passo passo, ma possiamo direttamente usare la funzione `lm` che stima modelli di regressione che minimizzano la somma dei quadrati:  

```{r lmCars, class.source = "fold-show", echo=TRUE}
# Italian keyboard: to do ~ press Alt+0126
lm(dist ~ speed, data = cars) 
```

Possiamo vedere quali sono i valori stimati per i coefficienti di regressione usando la funzione `coef`

```{r}
fit <- lm(dist ~ speed, data = cars) 
coef(fit)
```


La linea stimata (e mostrata in figura) è la retta che minimizza la somma dei quadrati:  

Possiamo infine ottenere molte informazioni sulla stima ottenuta usando la funzione `summary`, che, quando applicata ad un oggetto di classe `lm` fornisce come output molte informazioni sulla stima ottenuta:

```{r}
summary(fit)
```

Vedremo nel corso delle prossime lezioni più dettagli sui vari componenti dell'output. 

Possiamo però già notare come R ci fornisca (`Residual standard error`) la stima di $\sigma$ - la deviazione standard della variabile casuale $Y|X$: 
\[s_e^2 = \frac{1}{n}\sum_{i=1}^{n} (y_i - \hat{y}_i)\]

Per calcolare $\hat{y}_i$ possiamo valutare la funzione $\beta_0 + \beta_1 x_i$: 

```{r}
# 
# also see ?fitted
```

La stima di $\sigma$ quindi è: 

```{r}
# sqrt()
```



